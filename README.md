# Mount Doom Challenge ğŸŒ‹ğŸ§ âš™ï¸

This repository implements a scalable, resilient system for processing voice agent transcripts using LLMs. ğŸŒğŸ”ğŸ“Š It demonstrates best practices in:

* **Asynchronous API Integration**: Authenticated streaming from an external transcripts API and submission of processed results.
* **Queue & Concurrency**: Back-pressure management with `asyncio.Queue` and a worker pool.
* **LLM Processing**: OpenAI integration for summarization and analysis with retries and prompt engineering.
* **Storage Layer**: Async persistence of raw transcripts and processed results in PostgreSQL via SQLAlchemy.
* **Testing & CI**: Comprehensive unit tests, integration tests using GitHub Actions, and code linting.
* **Containerization**: Dockerfile and Docker Compose for easy local deployment.

## Repository Structure ğŸ—‚ï¸ğŸ“ğŸ”

```
mount-doom-challenge/
â”‚
â”œâ”€â”€ README.md                 # Challenge overview and setup guide
â”œâ”€â”€ CHALLENGE.md              # Detailed prompt description
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ api-schema.md         # API documentation
â”‚   â””â”€â”€ system-diagram.svg    # Architecture diagram
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ processing/
â”‚   â”œâ”€â”€ queue/
â”‚   â”œâ”€â”€ storage/
â”‚   â””â”€â”€ app.py                # Main entrypoint
â”‚
â”œâ”€â”€ tests/                    # Test suite
â”œâ”€â”€ scripts/                  # Utility scripts
â”œâ”€â”€ Dockerfile                # Container definition
â”œâ”€â”€ docker-compose.yml        # Service orchestration
â””â”€â”€ .github/workflows/ci.yml  # CI pipeline
```

## Prerequisites âœ…ğŸ§°ğŸ”‘

* Docker & Docker Compose
* Python 3.11
* An OpenAI API key (set `OPENAI_API_KEY`)

## Local Development ğŸ’»ğŸ”§ğŸš€

1. **Clone the repo**

   ```bash
   git clone https://github.com/PoornaSaiNagendra/VOICE-AGENT.git
   cd VOICE-AGENT
   ```

2. **Environment Variables**
   Create a `.env` file or export:

   ```bash
   export API_KEY="<your-api-key>"
   export BASE_URL="https://<your-api>/api"
   export OPENAI_API_KEY="<your-openai-key>"
   ```

3. **Start Services**

   ```bash
   docker-compose up --build
   ```

4. **Run Tests**

   ```bash
   pytest
   ```

## Deployment ğŸ“¦ğŸš¢ğŸŒ

* Build Docker image: `docker build -t mount-doom-challenge:latest .`
* Push to your registry and deploy with Kubernetes, ECS, or other container orchestrators.

## Extending the System ğŸ§©ğŸ”„ğŸ“ˆ

* **LLM Models**: Swap `gpt-4o-mini` with other OpenAI models or integrate additional providers.
* **Scaling**: Adjust `CONCURRENCY` and queue parameters in `src/app.py`.
* **Observability**: Integrate Prometheus, Grafana, and distributed tracing.


